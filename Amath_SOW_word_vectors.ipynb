{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys\n",
    "import numpy as np\n",
    "from heapq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(filename):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8', newline='\\n')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.asarray(list(map(float, tokens[1:])))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ** Word vectors ** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading word vectors\n",
    "\n",
    "print('')\n",
    "print(' ** Word vectors ** ')\n",
    "print('')\n",
    "\n",
    "word_vectors = load_vectors('wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function computes the cosine similarity between vectors u and v\n",
    "\n",
    "def cosine(u, v):\n",
    "    num = u.dot(v)\n",
    "    denom = np.linalg.norm(u)*np.linalg.norm(v)\n",
    "    return num/denom \n",
    "\n",
    "## This function returns the word corresponding to \n",
    "## nearest neighbor vector of x\n",
    "## The list exclude_words can be used to exclude some\n",
    "## words from the nearest neighbors search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(apple, apples) = 0.637\n",
      "similarity(apple, banana) = 0.431\n",
      "similarity(apple, tiger) = 0.212\n"
     ]
    }
   ],
   "source": [
    "# compute similarity between words\n",
    "\n",
    "print('similarity(apple, apples) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['apples']))\n",
    "print('similarity(apple, banana) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['banana']))\n",
    "print('similarity(apple, tiger) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['tiger']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for nearest neighbors\n",
    "def nearest_neighbor(x, word_vectors, exclude_words=[]):\n",
    "    best_score = -1.0\n",
    "    best_word = ''\n",
    "    for word in word_vectors:\n",
    "        if word not in exclude_words:\n",
    "            sim = cosine(x,word_vectors[word])\n",
    "        if sim > best_score:\n",
    "            best_score = sim\n",
    "            best_word = word\n",
    "    return best_word\n",
    "\n",
    "## This function return the words corresponding to the\n",
    "## K nearest neighbors of vector x.\n",
    "## You can use the functions heappush and heappop.\n",
    "\n",
    "def knn(x, vectors, k):\n",
    "    heap = []\n",
    "    best_score = -1\n",
    "    for word, vector in vectors.items():\n",
    "        if (x!=vector).all():\n",
    "            sim = cosine(x,vector)\n",
    "            heappush(heap,(sim,word))\n",
    "        if len(heap)> k:\n",
    "            heappop(heap)\n",
    "\n",
    "    return [heappop(heap) for i in range(len(heap))][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest neighbor of cat is: cat\n",
      "\n",
      "cat\n",
      "--------------\n",
      "cats\t0.732\n",
      "dog\t0.638\n",
      "pet\t0.573\n",
      "rabbit\t0.549\n",
      "dogs\t0.538\n"
     ]
    }
   ],
   "source": [
    "# looking at nearest neighbors of a word\n",
    "\n",
    "print('The nearest neighbor of cat is: ' +\n",
    "      nearest_neighbor(word_vectors['cat'], word_vectors))\n",
    "\n",
    "knn_cat = knn(word_vectors['cat'], word_vectors, 5)\n",
    "print('')\n",
    "print('cat')\n",
    "print('--------------')\n",
    "for score, word in knn(word_vectors['cat'], word_vectors, 5):\n",
    "    print(word + '\\t%.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(a,b,c,word_vectors):\n",
    "    ## FILL CODE\n",
    "    a, b, c= a.lower(), b.lower(), c.lower () \n",
    "    # find the word embeddings for word_a, word_b, word_c \n",
    "    e_a, e_b, e_c = word_vectors[a], word_vectors[b], word_vectors[c] \n",
    "    words = word_vectors.keys() \n",
    "    max_cosine_sim = -np.inf \n",
    "    best_word = None \n",
    "    # search for d in the whole word vector set \n",
    "    for w in words:\n",
    "        # ignore input words \n",
    "        if w in [a, b, c]: \n",
    "            continue \n",
    "        # Compute cosine similarity between the vectors u and v \n",
    "        cos_sim = cosine(e_b - e_a, word_vectors[w] - e_c) \n",
    "        if cos_sim> max_cosine_sim: \n",
    "            max_cosine_sim = cos_sim \n",
    "            # update word_d \n",
    "            best_word = w\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "france - paris + rome = italy\n"
     ]
    }
   ],
   "source": [
    "# Word analogies\n",
    "\n",
    "print('')\n",
    "print('france - paris + rome = ' + analogy('paris', 'france', 'rome', word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman = spider\n"
     ]
    }
   ],
   "source": [
    "print('king - man + woman = ' + analogy('king', 'man', 'woman', word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "similarity(genius, man) = 0.445\n",
      "similarity(genius, woman) = 0.325\n"
     ]
    }
   ],
   "source": [
    "## A word about biases in word vectors:\n",
    "\n",
    "print('')\n",
    "print('similarity(genius, man) = %.3f' %\n",
    "      cosine(word_vectors['man'], word_vectors['genius']))\n",
    "print('similarity(genius, woman) = %.3f' %\n",
    "      cosine(word_vectors['woman'], word_vectors['genius']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the association strength between:\n",
    "##   - a word w\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def association_strength(w, A, B, vectors):\n",
    "    elt_a = 0\n",
    "    elt_b = 0\n",
    "    for a in A:\n",
    "        elt_a += cosine(w,vectors[a])\n",
    "    for b in B:\n",
    "        elt_b += cosine(w,vectors[b])\n",
    "    return 1/len(A)*elt_a - 1/len(B)*elt_b\n",
    "\n",
    "## Perform the word embedding association test between:\n",
    "##   - two sets of words X and Y\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def weat(X, Y, A, B, vectors):\n",
    "    score = 0.0\n",
    "    sum_x=sum_y=0\n",
    "    for x in X:\n",
    "        sum_x +=association_strength(vectors[x],A,B,vectors)\n",
    "    for y in Y:\n",
    "        sum_y +=association_strength(vectors[y],A,B,vectors)   \n",
    "    \n",
    "    score = sum_x - sum_y\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embedding association test: 0.847\n"
     ]
    }
   ],
   "source": [
    "## Replicate one of the experiments from:\n",
    "##\n",
    "## Semantics derived automatically from language corpora contain human-like biases\n",
    "## Caliskan, Bryson, Narayanan (2017)\n",
    "\n",
    "career = ['executive', 'management', 'professional', 'corporation', \n",
    "          'salary', 'office', 'business', 'career']\n",
    "family = ['home', 'parents', 'children', 'family',\n",
    "          'cousins', 'marriage', 'wedding', 'relatives']\n",
    "male = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
    "female = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
    "\n",
    "print('')\n",
    "print('Word embedding association test: %.3f' %\n",
    "      weat(career, family, male, female, word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
